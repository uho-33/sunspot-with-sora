{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总亮度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化方法：\n",
    "- 假设服从平方反比，标准到 $1AU$ 的结果\n",
    "- 所有数据计算得到平均值 $m$ ，标准差 $sigma$\n",
    "- 标准化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import sunpy.map\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from glob import glob\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gc  # Add garbage collection\n",
    "import time  # For timing stats\n",
    "\n",
    "# Configure logging\n",
    "log_dir = \"Data-process\"\n",
    "log_file = Path(log_dir) / \"processing.log\"\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def normalize_distance(data_map):\n",
    "    \"\"\"Normalize the data based on distance to Sun in AU.\"\"\"\n",
    "    data_out = data_map.data * (data_map.dsun.to(u.AU)/(1 * u.AU).value)**2\n",
    "    return data_out\n",
    "\n",
    "def save_as_hdf5(data, output_path):\n",
    "    \"\"\"Save normalized data as HDF5 file.\"\"\"\n",
    "    with h5py.File(output_path, 'w') as hf:\n",
    "        hf.create_dataset('data', data=data, compression=\"gzip\", compression_opts=9)\n",
    "\n",
    "def is_processed(input_path, output_dir):\n",
    "    \"\"\"Check if a file has already been processed.\"\"\"\n",
    "    base_name = Path(input_path).stem\n",
    "    hdf5_path = Path(output_dir) / f\"{base_name}_nd.h5\"\n",
    "    return hdf5_path.exists()\n",
    "\n",
    "def get_output_path(input_path, output_dir):\n",
    "    \"\"\"Get the output HDF5 path for a given input file.\"\"\"\n",
    "    base_name = Path(input_path).stem\n",
    "    return Path(output_dir) / f\"{base_name}_nd.h5\"\n",
    "\n",
    "def process_fits_file(input_path, output_dir, allow_errors=False):\n",
    "    \"\"\"Process a FITS file and save normalized data as HDF5.\"\"\"\n",
    "    try:\n",
    "        # Load and process data\n",
    "        data_map = sunpy.map.Map(input_path)\n",
    "        normalized_data = normalize_distance(data_map)\n",
    "        \n",
    "        # Generate output filename and save\n",
    "        hdf5_path = get_output_path(input_path, output_dir)\n",
    "        save_as_hdf5(normalized_data, hdf5_path)\n",
    "        \n",
    "        # Explicitly clear references to free memory\n",
    "        del data_map\n",
    "        del normalized_data\n",
    "        gc.collect()  # Force garbage collection\n",
    "        \n",
    "        # logging.info(f\"Saved HDF5 file: {hdf5_path}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {input_path}: {e}\")\n",
    "        if not allow_errors:\n",
    "            raise\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    os.chdir(r\"E:\\study-and-research\\sunspot-sith-sora\")\n",
    "\n",
    "    # Get all FITS files in the input directory\n",
    "    input_dir = Path(r\"data\\origin\\Ic_720s\")\n",
    "    output_dir = Path(r\"data\\processed\\Ic_720s_normalize_distance\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Pre-filter files to process\n",
    "    fits_files = list(input_dir.glob(\"*.fits\"))\n",
    "    total_files = len(fits_files)\n",
    "    \n",
    "    # Pre-check which files need processing\n",
    "    files_to_process = []\n",
    "    skipped_files = 0\n",
    "    \n",
    "    logging.info(f\"Checking {total_files} files for processing status...\")\n",
    "    \n",
    "    for file in fits_files:\n",
    "        if is_processed(file, output_dir):\n",
    "            skipped_files += 1\n",
    "        else:\n",
    "            files_to_process.append(file)\n",
    "    \n",
    "    need_processing = len(files_to_process)\n",
    "    logging.info(f\"Found {need_processing} files to process, {skipped_files} already processed\")\n",
    "    \n",
    "    # Counter for processed files\n",
    "    processed_files = 0\n",
    "    error_files = 0\n",
    "\n",
    "    # Process files in parallel with optimal workers\n",
    "    max_workers = 10\n",
    "    \n",
    "    if files_to_process:\n",
    "        logging.info(f\"Starting processing with {max_workers} workers\")\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Process files in batches to prevent memory buildup\n",
    "            batch_size = 10\n",
    "            for i in range(0, len(files_to_process), batch_size):\n",
    "                batch = files_to_process[i:i+batch_size]\n",
    "                # logging.info(f\"Processing batch {i//batch_size + 1} of {(len(files_to_process)-1)//batch_size + 1} ({len(batch)} files)\")\n",
    "                \n",
    "                # Submit batch for processing\n",
    "                futures = [\n",
    "                    executor.submit(process_fits_file, str(file), output_dir, True)\n",
    "                    for file in batch\n",
    "                ]\n",
    "                \n",
    "                # Process results\n",
    "                for future in futures:\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        if result is True:\n",
    "                            processed_files += 1\n",
    "                        else:\n",
    "                            error_files += 1\n",
    "                    except Exception as e:\n",
    "                        error_files += 1\n",
    "                        logging.error(f\"Failed to process a file: {e}\")\n",
    "                \n",
    "                # Ensure memory is freed between batches\n",
    "                gc.collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    logging.info(f\"Processing complete in {elapsed_time:.2f} seconds:\")\n",
    "    logging.info(f\"Total files: {total_files}\")\n",
    "    logging.info(f\"Already processed (skipped): {skipped_files}\")\n",
    "    logging.info(f\"Successfully processed: {processed_files}\")\n",
    "    logging.info(f\"Failed to process: {error_files}\")\n",
    "    \n",
    "    if processed_files > 0:\n",
    "        logging.info(f\"Average processing time per file: {elapsed_time/processed_files:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "os.chdir(r\"E:\\study-and-research\\sunspot-with-sora\")\n",
    "\n",
    "# Directory containing the processed h5 files\n",
    "data_dir = Path(\"data/processed/Ic_720s_normalize_distance\")\n",
    "\n",
    "# Get all h5 files\n",
    "h5_files = list(data_dir.glob(\"*.h5\"))\n",
    "print(f\"Found {len(h5_files)} h5 files\")\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    \"\"\"\n",
    "    Extract timestamp from the filename.\n",
    "    Assumes filename format contains date/time information.\n",
    "    Modify the regex pattern based on your actual filename format.\n",
    "    \"\"\"\n",
    "    # Example pattern: looking for something like YYYY-MM-DD_HH-MM-SS\n",
    "    pattern = r'(\\d{4}-\\d{2}-\\d{2}[_T]\\d{2}[-:]\\d{2}[-:]\\d{2})'\n",
    "    match = re.search(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        # Convert matched string to datetime\n",
    "        dt_str = match.group(1).replace('_', 'T').replace('-', ':')\n",
    "        return datetime.datetime.fromisoformat(dt_str)\n",
    "    \n",
    "    # Fallback: use file modification time if pattern doesn't match\n",
    "    file_stat = os.stat(filename)\n",
    "    return datetime.datetime.fromtimestamp(file_stat.st_mtime)\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Process a single file to extract timestamp and calculate brightness.\"\"\"\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as hf:\n",
    "            # Get data and calculate brightness\n",
    "            data = hf['data'][:]\n",
    "            brightness = np.nansum(data)\n",
    "            \n",
    "            # Try to get timestamp from file metadata if available\n",
    "            timestamp = None\n",
    "            if 'timestamp' in hf.attrs:\n",
    "                timestamp = hf.attrs['timestamp']\n",
    "            else:\n",
    "                # Extract from filename\n",
    "                timestamp = extract_timestamp(file_path)\n",
    "                \n",
    "            return timestamp, brightness, file_path.name\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Calculate total brightness for each file in parallel\n",
    "print(\"Calculating total brightness for each file in parallel...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Define number of workers based on CPU cores\n",
    "num_workers = os.cpu_count() - 1  # Leave one core free\n",
    "if num_workers < 1:\n",
    "    num_workers = 1\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {executor.submit(process_file, file_path): file_path for file_path in h5_files}\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "        file_path = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Parallel processing completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Convert results to a DataFrame for easier manipulation\n",
    "brightness_df = pd.DataFrame(results, columns=['timestamp', 'brightness', 'filename'])\n",
    "brightness_df = brightness_df.sort_values('timestamp')\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = \"data/processed/total_brightness_timeseries.csv\"\n",
    "brightness_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved time series data to {csv_path}\")\n",
    "\n",
    "# Also save to numpy format\n",
    "np_path = \"data/processed/total_brightness_timeseries.npz\"\n",
    "np.savez(np_path, \n",
    "         timestamps=brightness_df['timestamp'].astype(np.datetime64).values,\n",
    "         brightness=brightness_df['brightness'].values,\n",
    "         filenames=brightness_df['filename'].values)\n",
    "print(f\"Saved numpy array to {np_path}\")\n",
    "\n",
    "# Display basic statistics\n",
    "brightness_array = brightness_df['brightness'].values\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(f\"Total number of files processed: {len(brightness_df)}\")\n",
    "print(f\"Mean total brightness: {np.mean(brightness_array):.2e}\")\n",
    "print(f\"Standard deviation: {np.std(brightness_array):.2e}\")\n",
    "print(f\"Min: {np.min(brightness_array):.2e}\")\n",
    "print(f\"Max: {np.max(brightness_array):.2e}\")\n",
    "\n",
    "# Plot time series data\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(brightness_df['timestamp'], brightness_df['brightness'])\n",
    "plt.title('Total Brightness Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Total Brightness')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/processed/brightness_timeseries.png')\n",
    "plt.show()\n",
    "\n",
    "# Continue with the sampling stability analysis\n",
    "# (keep your existing code for this part)\n",
    "sample_sizes = [10, 50, 100, 200, 500, 1000, 2000, 5000]\n",
    "if len(brightness_df) < 5000:\n",
    "    # Adjust sample sizes if we have fewer data points\n",
    "    sample_sizes = [s for s in sample_sizes if s <= len(brightness_df)]\n",
    "    if len(sample_sizes) == 0:\n",
    "        sample_sizes = [1, 5, 10, len(brightness_df)//2, len(brightness_df)]\n",
    "\n",
    "num_experiments = 100  # Number of random sampling experiments for each size\n",
    "sampling_results = []\n",
    "\n",
    "for sample_size in tqdm(sample_sizes):\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for _ in range(num_experiments):\n",
    "        if sample_size < len(brightness_array):\n",
    "            sample = np.random.choice(brightness_array, size=sample_size, replace=False)\n",
    "        else:\n",
    "            sample = brightness_array\n",
    "            \n",
    "        means.append(np.mean(sample))\n",
    "        stds.append(np.std(sample))\n",
    "    \n",
    "    sampling_results.append({\n",
    "        'sample_size': sample_size,\n",
    "        'mean_of_means': np.mean(means),\n",
    "        'std_of_means': np.std(means),\n",
    "        'mean_of_stds': np.mean(stds)\n",
    "    })\n",
    "\n",
    "# Rest of your plotting code as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化方法：\n",
    "- 随机化计算有效数据点数的平均值 $n$\n",
    "- 计算标准化系数：$m_f=1$ , $sigma_f=sigma/m$ （考虑到 nolimbdark 数据为相对值，用 m 代替平静太阳的值）\n",
    "- 选取 $4 sigma$ 内结果映射到 $[0,255]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种处理方法，采用映射 $e^{-x+1}$ 在 1 附近近似线性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import sunpy.map\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%cd \"E:/study-and-research/sunspot-with-sora\"\n",
    "data_in = sunpy.map.Map(\n",
    "    r\"data\\origin\\flare_Ic_nolimbdark_720s\\hmi.ic_nolimbdark_720s.20240222_200000_TAI.3.continuum.fits\"\n",
    ")\n",
    "data_in2 = sunpy.map.Map(r\"data\\origin\\Ic_720s\\hmi.ic_720s.20200601_000000_TAI.3.continuum.fits\")\n",
    "\n",
    "np.nanmin(data_in.data)\n",
    "\n",
    "data_in2.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: 验证 DN/s 是否服从平方反比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downsample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "%cd \"E:\\study-and-research\\sunspot-with-sora\"\n",
    "# Load image\n",
    "image = Image.open(r\"data\\processed\\figure\\figure-origin\\hmi.ic_nolimbdark_720s.20211101_000000_TAI.3.continuum_nd.png\")\n",
    "\n",
    "# Resize using Lanczos\n",
    "resized_image = image.resize((960, 960), Image.LANCZOS)\n",
    "\n",
    "# Save or show the result\n",
    "resized_image.save(r\"data\\processed\\figure\\figure-downsample\\hmi_ic_resized.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用滑动窗口形式构造 dataset\n",
    "\n",
    "TODO 实验不同窗口大小与窗口间隔的影响\n",
    "\n",
    "初步采用：\n",
    "窗口大小：16（~1/4太阳周）\n",
    "间隔：12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import sunpy.map\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%cd \"E:\\study-and-research\\sunspot-with-sora\"\n",
    "data_in = sunpy.map.Map(\n",
    "    \"hmi.ic_720s.2024-02-20T00_10_32.3Z.838005.continuum.fits\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils.fits_tools import fix_and_load_fits\n",
    "\n",
    "%cd \"E:\\study-and-research\\sunspot-with-sora\"\n",
    "# Load the FITS file with fixed metadata\n",
    "data_in = fix_and_load_fits(\"Data-process/hmi.ic_720s.6173.831698.2024-01-01T12_10_29.700Z.continuum.fits\")\n",
    "\n",
    "# Now you can work with the data_in SunPy map\n",
    "plt.figure(figsize=(10, 10))\n",
    "data_in.plot()\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import sunpy.map\n",
    "\n",
    "fits_file = \"Data-process\\hmi.ic_720s.6173.831698.2024-01-01T12_10_29.700Z.continuum.fits\"\n",
    "data_in = sunpy.map.Map(fits_file, {**fits.getheader(fits_file), 'CUNIT1': 'arcsec', 'CUNIT2': 'arcsec'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in.meta.get('BUNIT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\study-and-research\\\\sunspot-with-sora\\\\dataset\\\\training\\\\brightness\\\\L16-S8\\\\20211101_000000.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mstudy-and-research\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msunspot-with-sora\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbrightness\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mL16-S8\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m20211101_000000.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m data2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32md:\\software\\Anaconda\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\study-and-research\\\\sunspot-with-sora\\\\dataset\\\\training\\\\brightness\\\\L16-S8\\\\20211101_000000.npz'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data2 = np.load(r'E:\\study-and-research\\sunspot-with-sora\\dataset\\training\\brightness\\L16-S8\\20211101_000000.npz')\n",
    "\n",
    "data2['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the brightness time series data\n",
    "data = np.load(r'E:\\study-and-research\\sunspot-with-sora\\data\\processed\\brightness\\filtered\\filtered_brightness_timeseries.npz')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the brightness time series data\n",
    "data = np.load(r'E:\\study-and-research\\sunspot-with-sora\\data\\processed\\brightness\\Ic_720s_dn_brightness_stats.npz')\n",
    "\n",
    "print(data['mean'])\n",
    "print(data['std'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
